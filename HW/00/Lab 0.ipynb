{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__szkOMBh8VS"
   },
   "source": [
    "# Lab 0: Getting familiar with the software environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PHjjdY-nh8VU"
   },
   "source": [
    "OpenAI gym environment setup. You may want to try this with other environments such as CartPole-v0. See https://gym.openai.com/envs/ for **many** other examples. The environment may not render here, but on a personal computer you should be able to see the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 867,
     "status": "ok",
     "timestamp": 1646547154208,
     "user": {
      "displayName": "­황태현",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08611587733822633308"
     },
     "user_tz": -540
    },
    "id": "xcB-2Mahh8VV",
    "outputId": "be50e97b-90fd-4b4b-e18e-dcd031b2b4ac"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "###추가###\n",
    "print(\"numpy version\", np.__version__)\n",
    "print(\"gym version\", gym.__version__)\n",
    "###추가###\n",
    "env=gym.make('FrozenLake-v0') #or 'FrozenLake-v1' depending on your PC\n",
    "env.reset()\n",
    "env.render() #For graphic environments like CartPole-v0 you will have to comment this here. \n",
    "             #But, on your personal computer, you should be able to render it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5sXtvemMh8VW"
   },
   "source": [
    "Below we illustrate the execution of the Open AI gym enviornment using the policy of chosing random action in every state. Every time an action is taken the enviorment returns a tuple containing next state, reward, and the status (whether terminal state is reached or not). The variable `reward` is recording the \"total reward\" of the policy over an episode of length 100. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G_yob7X2h8VX"
   },
   "outputs": [],
   "source": [
    "rList = [] #lost oftotal reward for each episodes\n",
    "num_episodes=500;\n",
    "episode_max_length=100;\n",
    "\n",
    "s=env.reset()\n",
    "#execute in episodes\n",
    "for i in range(num_episodes):\n",
    "    d = False #not done\n",
    "    reward=0\n",
    "    for t in range(episode_max_length):\n",
    "        \n",
    "        ################YOUR CODE HERE################\n",
    "        #play random action \n",
    "        a = env.action_space.sample()\n",
    "        #get new state, reward, done\n",
    "        s, r, d, _ = env.step(a)\n",
    "        reward+=r\n",
    "        #break if done, reached terminal state \n",
    "        if d == True:\n",
    "            break\n",
    "    #reset the environment for the next episode\n",
    "    s = env.reset()\n",
    "    #reward for the episode\n",
    "    rList.append(reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9HhuD2kyh8VX"
   },
   "source": [
    "The code below analyzes how this random policy performs. It plots moving average of reward over 100 consecutive episodes.\n",
    "This most likely does not perform very well. You can try playing with some other simple policies and see how they perform for different environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eYZd8E8Nh8VY"
   },
   "outputs": [],
   "source": [
    "from numpy import convolve, ones\n",
    "def movingaverage(interval, window_size):\n",
    "    window= np.ones(int(window_size))/float(window_size)\n",
    "    return np.convolve(interval, window, 'valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "executionInfo": {
     "elapsed": 994,
     "status": "ok",
     "timestamp": 1646547161647,
     "user": {
      "displayName": "­황태현",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08611587733822633308"
     },
     "user_tz": -540
    },
    "id": "hI_wZRgJh8VY",
    "outputId": "3d543574-8eed-4ec3-b944-dbf2730a2ca6"
   },
   "outputs": [],
   "source": [
    "from pylab import plot\n",
    "%matplotlib inline \n",
    "rm=movingaverage(rList, 100)\n",
    "plot(rm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X1l46_9vh8VZ"
   },
   "source": [
    "In your programming assignments, you will be given a skeleton of code like above. You will be asked to make changes to the code to achieve specific tasks and then submit the notebook as your submission for the assignment. Save this notebook file (after you run the code) and submit \"Lab 0.ipynb\" on eTL. This will not be graded but will help us ensure that everything is set up correctly on your machine.\n",
    "\n",
    "If you have any trouble on running this code on your machine, I recommend you to use the Google Colab.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JN1KwotCh8VZ"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab 0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
